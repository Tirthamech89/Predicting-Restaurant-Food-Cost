{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('Data_Train.xlsx')\n",
    "test = pd.read_excel('Data_Test.xlsx')\n",
    "\n",
    "tg=train[['COST']]\n",
    "tg[\"COST1\"] = np.log1p(tg[\"COST\"])\n",
    "target=tg.COST1\n",
    "del train['COST']\n",
    "del train['RESTAURANT_ID']\n",
    "del test['RESTAURANT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12690, 7)\n",
      "(4231, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data=pd.concat([train,test])\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['TITLE']=all_data['TITLE'].str.strip()\n",
    "all_data['TITLE']=all_data['TITLE'].str.upper()\n",
    "\n",
    "all_data['CUISINES']=all_data['CUISINES'].str.strip()\n",
    "all_data['CUISINES']=all_data['CUISINES'].str.upper()\n",
    "\n",
    "all_data['CITY']=all_data['CITY'].str.strip()\n",
    "all_data['CITY']=all_data['CITY'].str.upper()\n",
    "\n",
    "all_data['LOCALITY']=all_data['LOCALITY'].str.strip()\n",
    "all_data['LOCALITY']=all_data['LOCALITY'].str.upper()\n",
    "\n",
    "all_data['TIME']=all_data['TIME'].str.strip()\n",
    "all_data['TIME']=all_data['TIME'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Rating\n",
    "\n",
    "rates = list(all_data['RATING'])\n",
    "\n",
    "for i in range(len(rates)) :\n",
    "    try:\n",
    "       rates[i] = float(rates[i])\n",
    "    except :\n",
    "       rates[i] = np.nan\n",
    "\n",
    "\n",
    "# Votes\n",
    "       \n",
    "votes = list(all_data['VOTES'])\n",
    "\n",
    "for i in range(len(votes)) :\n",
    "    try:\n",
    "       votes[i] = int(votes[i].split(\" \")[0].strip())\n",
    "    except :\n",
    "       pass     \n",
    "\n",
    "num_data = {}\n",
    "\n",
    "num_data['RATING'] = rates\n",
    "num_data['VOTES'] = votes\n",
    "\n",
    "num_data = pd.DataFrame(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_data['RATING']\n",
    "del all_data['VOTES']\n",
    "feature_data=pd.concat([all_data,num_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TITLE', 'CUISINES', 'TIME', 'CITY', 'LOCALITY']\n",
      "['RATING', 'VOTES']\n"
     ]
    }
   ],
   "source": [
    "cat_cols=feature_data.columns[feature_data.dtypes=='object'].tolist()\n",
    "print (cat_cols)\n",
    "\n",
    "num_cols=feature_data.columns[feature_data.dtypes!='object'].tolist()\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_num_feats=pd.DataFrame(np.column_stack([feature_data[m[0]].map(dict(feature_data.groupby(m[0])[m[1]].mean()))\n",
    " for m in [(a,b) for a in cat_cols for b in num_cols]]),\n",
    "columns=['cat_num_feat'+str(i) for i in range(len(cat_cols)*len(num_cols))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_data.reset_index(drop=True,inplace=True)\n",
    "feature_data=pd.concat((feature_data,cat_num_feats),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoding(BaseEstimator):\n",
    "    categorical_columns = None\n",
    "    return_df = False\n",
    "    random_state = 30\n",
    "    threshold = 50\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert_input(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            if isinstance(X, list):\n",
    "                X = pd.DataFrame(np.array(X))\n",
    "            elif isinstance(X, (np.generic, np.ndarray, pd.Series)):\n",
    "                X = pd.DataFrame(X)\n",
    "            else:\n",
    "                raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "            X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "        x = X.copy(deep = True)\n",
    "        return x\n",
    "\n",
    "    def get_categorical_columns(self, X):\n",
    "        return X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    def get_numerical_columns(self,X):\n",
    "        temp_x=X[X.columns[X.nunique()<=self.threshold]]\n",
    "        col_names=temp_x.columns[temp_x.dtypes!='object']\n",
    "        return col_names\n",
    "\n",
    "    def apply_encoding(self, X_in, encoding_dict):\n",
    "        X = self.convert_input(X_in)\n",
    "        for col in self.categorical_columns:\n",
    "            if col in encoding_dict:\n",
    "                freq_dict = encoding_dict[col]\n",
    "                X[col] = X[col].apply(lambda x: freq_dict[x] if x  in freq_dict else np.nan)\n",
    "        return X\n",
    "\n",
    "    def create_encoding_dict(self, X, y):\n",
    "        return {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if X is None:\n",
    "            raise ValueError(\"Input array is required to call fit method!\")\n",
    "        X = self.convert_input(X)\n",
    "        self.encoding_dict = self.create_encoding_dict(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = self.apply_encoding(X, self.encoding_dict)\n",
    "        if self.return_df:\n",
    "            return df\n",
    "        else:\n",
    "            return df.values\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X = self.convert_input(X)\n",
    "        for col in self.categorical_columns:\n",
    "            freq_dict = self.encoding_dict[col]\n",
    "            for key, val in freq_dict.iteritems():\n",
    "                X.loc[X[col] == val, col] = key\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FreqeuncyEncoding(Encoding):\n",
    "    '''\n",
    "    class to perform FreqeuncyEncoding on Categorical Variables\n",
    "    Initialization Variabes:\n",
    "    categorical_columns: list of categorical columns from the dataframe\n",
    "    or list of indexes of caategorical columns for numpy ndarray\n",
    "    return_df: boolean\n",
    "        if True: returns pandas dataframe on transformation\n",
    "        else: return numpy ndarray\n",
    "    '''\n",
    "    def __init__(self, categorical_columns = None, return_df = False):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.return_df = return_df\n",
    "\n",
    "    def create_encoding_dict(self, X, y):\n",
    "        encoding_dict = {}\n",
    "        if self.categorical_columns is None:\n",
    "            self.categorical_columns = self.get_categorical_columns(X)\n",
    "        for col in self.categorical_columns:\n",
    "            encoding_dict.update({col: X[col].value_counts(normalize = True).to_dict()})\n",
    "        return encoding_dict\n",
    "\n",
    "\n",
    "fe=FreqeuncyEncoding(categorical_columns=cat_cols,return_df=True)\n",
    "feature_data1=fe.fit_transform(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximum Titles in a Cell :  2\n",
      "\n",
      "\n",
      "Number of Unique Titles :  25\n",
      "\n",
      "\n",
      "Unique Titles:\n",
      " ['CASUAL DINING' 'BAR' 'QUICK BITES' 'DESSERT PARLOR' 'CAFE'\n",
      " 'MICROBREWERY' 'BEVERAGE SHOP' 'IRANI CAFE' 'BAKERY' 'NONE' 'PUB'\n",
      " 'FINE DINING' 'SWEET SHOP' 'LOUNGE' 'FOOD COURT' 'FOOD TRUCK' 'MESS'\n",
      " 'KIOSK' 'CLUB' 'CONFECTIONERY' 'DHABA' 'MEAT SHOP' 'COCKTAIL BAR'\n",
      " 'PAAN SHOP' 'BHOJANALYA']\n"
     ]
    }
   ],
   "source": [
    "titles = list(all_data['TITLE'])\n",
    "\n",
    "# Finding Maximum number of titles mentioned in a single cell\n",
    "maxim = 1\n",
    "for i in titles :\n",
    "    if len(i.split(',')) > maxim:\n",
    "         maxim = len(i.split(','))\n",
    "         \n",
    "print(\"\\n\\nMaximum Titles in a Cell : \", maxim)    \n",
    "\n",
    "all_titles = []\n",
    "\n",
    "for i in titles :\n",
    "    if len(i.split(',')) == 1:\n",
    "         all_titles.append(i.split(',')[0].strip().upper())\n",
    "    else :\n",
    "        for it in range(len(i.split(','))):\n",
    "            all_titles.append(i.split(',')[it].strip().upper())\n",
    "\n",
    "print(\"\\n\\nNumber of Unique Titles : \", len(pd.Series(all_titles).unique()))\n",
    "print(\"\\n\\nUnique Titles:\\n\", pd.Series(all_titles).unique())\n",
    "\n",
    "all_titles = list(pd.Series(all_titles).unique())\n",
    "\n",
    "\n",
    "for i in range(25):\n",
    "    ttl=all_titles[i]\n",
    "    ttl1=ttl+'1'\n",
    "    all_data[ttl1] = all_data['TITLE'].str.contains(ttl)\n",
    "    all_data[ttl1] = all_data[ttl1].map({True: 1, False: 0})\n",
    "    \n",
    "    \n",
    "#del all_data['TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximum cuisines in a Cell :  8\n",
      "\n",
      "\n",
      "Number of Unique Cuisines :  130\n",
      "\n",
      "\n",
      "Unique Cuisines:\n",
      " ['MALWANI' 'GOAN' 'NORTH INDIAN' 'ASIAN' 'MODERN INDIAN' 'JAPANESE'\n",
      " 'CHINESE' 'BIRYANI' 'HYDERABADI' 'TIBETAN' 'DESSERTS' 'SEAFOOD' 'CAFE'\n",
      " 'PIZZA' 'BURGER' 'BAR FOOD' 'SOUTH INDIAN' 'FAST FOOD' 'BEVERAGES'\n",
      " 'ARABIAN' 'MUGHLAI' 'MAHARASHTRIAN' 'PARSI' 'THAI' 'BAKERY' 'MOMOS'\n",
      " 'CONTINENTAL' 'EUROPEAN' 'ROLLS' 'ANDHRA' 'ITALIAN' 'BBQ' 'FINGER FOOD'\n",
      " 'TEA' 'AMERICAN' 'HEALTHY FOOD' 'COFFEE' 'INDONESIAN' 'KOREAN' 'NEPALESE'\n",
      " 'ICE CREAM' 'MEXICAN' 'KERALA' 'INDIAN' 'MITHAI' 'STREET FOOD'\n",
      " 'MALAYSIAN' 'VIETNAMESE' 'IRANIAN' 'KEBAB' 'JUICES' 'SANDWICH'\n",
      " 'MEDITERRANEAN' 'SALAD' 'GUJARATI' 'RAJASTHANI' 'TEX-MEX' 'ROAST CHICKEN'\n",
      " 'BURMESE' 'CHETTINAD' 'NORTH EASTERN' 'LEBANESE' 'COFFEE AND TEA' 'GRILL'\n",
      " '' 'BIHARI' 'BENGALI' 'LUCKNOWI' 'AWADHI' 'STEAK' 'FRENCH' 'PORTUGUESE'\n",
      " 'WRAPS' 'SRI LANKAN' 'ORIYA' 'ETHIOPIAN' 'KONKAN' 'SUSHI' 'SPANISH'\n",
      " 'RUSSIAN' 'MANGALOREAN' 'TURKISH' 'BUBBLE TEA' 'AFGHAN' 'NAGA'\n",
      " 'SINGAPOREAN' 'GERMAN' 'MIDDLE EASTERN' 'SINDHI' 'CANTONESE' 'HOT POT'\n",
      " 'PAN ASIAN' 'SATAY' 'DUMPLINGS' 'KASHMIRI' 'RAW MEATS' 'DRINKS ONLY'\n",
      " 'MOROCCAN' 'PANINI' 'CAFE FOOD' 'CHARCOAL CHICKEN' 'BELGIAN' 'MONGOLIAN'\n",
      " 'TAMIL' 'AFRICAN' 'PAAN' 'ASSAMESE' 'HOT DOGS' 'POKE' 'BRITISH' 'BOHRI'\n",
      " 'FUSION' 'ARMENIAN' 'SOUTH AMERICAN' 'GREEK' 'PAKISTANI' 'PERUVIAN'\n",
      " 'CUISINE VARIES' 'IRISH' 'MULTI CUISINE' 'JEWISH' 'VEGAN' 'ORIENTAL'\n",
      " 'MODERN AUSTRALIAN' 'EGYPTIAN' 'FISH AND CHIPS' 'BRAZILIAN' 'MISHTI'\n",
      " 'FALAFEL' 'HAWAIIAN']\n"
     ]
    }
   ],
   "source": [
    "# Analysing cuisines \n",
    "\n",
    "cuisines = list(all_data['CUISINES'])\n",
    "\n",
    "maxim = 1\n",
    "for i in cuisines :\n",
    "    if len(i.split(',')) > maxim:\n",
    "         maxim = len(i.split(','))\n",
    "         \n",
    "print(\"\\n\\nMaximum cuisines in a Cell : \", maxim)    \n",
    "\n",
    "all_cuisines = []\n",
    "\n",
    "for i in cuisines :\n",
    "    if len(i.split(',')) == 1:\n",
    "         #print(i.split(',')[0])\n",
    "         all_cuisines.append(i.split(',')[0].strip().upper())\n",
    "    else :\n",
    "        for it in range(len(i.split(','))):\n",
    "            #print(i.split(',')[it])\n",
    "            all_cuisines.append(i.split(',')[it].strip().upper())\n",
    "\n",
    "print(\"\\n\\nNumber of Unique Cuisines : \", len(pd.Series(all_cuisines).unique()))\n",
    "print(\"\\n\\nUnique Cuisines:\\n\", pd.Series(all_cuisines).unique())\n",
    "\n",
    "all_cuisines = list(pd.Series(all_cuisines).unique())\n",
    "\n",
    "for i in range(130):\n",
    "    ttl=all_cuisines[i]\n",
    "    ttl2=ttl+'2'\n",
    "    all_data[ttl2] = all_data['CUISINES'].str.contains(ttl)\n",
    "    all_data[ttl2] = all_data[ttl2].map({True: 1, False: 0})\n",
    "    \n",
    "    \n",
    "#del all_data['CUISINES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16921, 160)\n"
     ]
    }
   ],
   "source": [
    "all_data=all_data.loc[:, (all_data != 0).any(axis=0)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  import sys\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "all_data['MON-TUE']=np.where(all_data['TIME'].str.contains('(MON-TUE)'),1,0)\n",
    "all_data['MON-WED']=np.where(all_data['TIME'].str.contains('(MON-WED)'),1,0)\n",
    "all_data['MON-THU']=np.where(all_data['TIME'].str.contains('(MON-THU)'),1,0)\n",
    "all_data['MON-FRI']=np.where(all_data['TIME'].str.contains('(MON-FRI)'),1,0)\n",
    "all_data['MON-SAT']=np.where(all_data['TIME'].str.contains('(MON-SAT)'),1,0)\n",
    "all_data['MON-SUN']=np.where(all_data['TIME'].str.contains('(MON-SUN)'),1,0)\n",
    "all_data['TUE-WED']=np.where(all_data['TIME'].str.contains('(TUE-WED)'),1,0)\n",
    "all_data['TUE-THU']=np.where(all_data['TIME'].str.contains('(TUE-THU)'),1,0)\n",
    "all_data['TUE-FRI']=np.where(all_data['TIME'].str.contains('(TUE-FRI)'),1,0)\n",
    "all_data['TUE-SAT']=np.where(all_data['TIME'].str.contains('(TUE-SAT)'),1,0)\n",
    "all_data['TUE-SUN']=np.where(all_data['TIME'].str.contains('(TUE-SUN)'),1,0)\n",
    "all_data['WED-THU']=np.where(all_data['TIME'].str.contains('(WED-THU)'),1,0)\n",
    "all_data['WED-FRI']=np.where(all_data['TIME'].str.contains('(WED-FRI)'),1,0)\n",
    "all_data['WED-SAT']=np.where(all_data['TIME'].str.contains('(WED-SAT)'),1,0)\n",
    "all_data['WED-SUN']=np.where(all_data['TIME'].str.contains('(WED-SUN)'),1,0)\n",
    "all_data['THU-FRI']=np.where(all_data['TIME'].str.contains('(THU-FRI)'),1,0)\n",
    "all_data['THU-SAT']=np.where(all_data['TIME'].str.contains('(THU-SAT)'),1,0)\n",
    "all_data['THU-SUN']=np.where(all_data['TIME'].str.contains('(THU-SUN)'),1,0)\n",
    "all_data['FRI-SAT']=np.where(all_data['TIME'].str.contains('(FRI-SAT)'),1,0)\n",
    "all_data['FRI-SUN']=np.where(all_data['TIME'].str.contains('(FRI-SUN)'),1,0)\n",
    "all_data['SAT-SUN']=np.where(all_data['TIME'].str.contains('(SAT-SUN)'),1,0)\n",
    "\n",
    "all_data['MON']=np.where(all_data['TIME'].str.contains('MON'),1,0)\n",
    "all_data['TUE']=np.where(all_data['TIME'].str.contains('TUE'),1,0)\n",
    "all_data['WED']=np.where(all_data['TIME'].str.contains('WED'),1,0)\n",
    "all_data['THU']=np.where(all_data['TIME'].str.contains('THU'),1,0)\n",
    "all_data['FRI']=np.where(all_data['TIME'].str.contains('FRI'),1,0)\n",
    "all_data['SAT']=np.where(all_data['TIME'].str.contains('SAT'),1,0)\n",
    "all_data['SUN']=np.where(all_data['TIME'].str.contains('SUN'),1,0)\n",
    "\n",
    "all_data['AM']=np.where(all_data['TIME'].str.contains('AM'),1,0)\n",
    "all_data['PM']=np.where(all_data['TIME'].str.contains('PM'),1,0)\n",
    "all_data['AM_cnt']=all_data['TIME'].str.count('AM')\n",
    "all_data['PM_cnt']=all_data['TIME'].str.count('PM')\n",
    "all_data['NOON']=np.where(all_data['TIME'].str.contains('NOON'),1,0)\n",
    "all_data['MIDNIGHT']=np.where(all_data['TIME'].str.contains('MIDNIGHT'),1,0)\n",
    "all_data['CLOSED']=np.where(all_data['TIME'].str.contains('CLOSED'),1,0)\n",
    "all_data['HOURS']=np.where(all_data['TIME'].str.contains('HOURS'),1,0)\n",
    "all_data['TIME_CNT']=all_data['TIME'].str.len()\n",
    "all_data['comma_count'] = all_data['TIME'].str.count(',')\n",
    "all_data['dash_count'] = all_data['TIME'].str.count('-')\n",
    "all_data['collon_count'] = all_data['TIME'].str.count(':')\n",
    "all_data['dotted_count'] = all_data['TIME'].str.count('...')\n",
    "#all_data['TIME_no_count'] = all_data['TIME'].str.count(r'[0-9]')\n",
    "#all_data['TIME_str_count'] = all_data['TIME'].str.count(r'[A-Z]')\n",
    "\n",
    "all_data['1:15AM']=np.where(all_data['TIME'].str.contains('1:15AM'),1,0)\n",
    "all_data['1:30AM']=np.where(all_data['TIME'].str.contains('1:30AM'),1,0)\n",
    "all_data['1:45AM']=np.where(all_data['TIME'].str.contains('1:45AM'),1,0)\n",
    "all_data['2AM']=np.where(all_data['TIME'].str.contains('2AM'),1,0)\n",
    "all_data['2:15AM']=np.where(all_data['TIME'].str.contains('2:15AM'),1,0)\n",
    "all_data['2:30AM']=np.where(all_data['TIME'].str.contains('2:30AM'),1,0)\n",
    "all_data['2:45AM']=np.where(all_data['TIME'].str.contains('2:45AM'),1,0)\n",
    "all_data['3AM']=np.where(all_data['TIME'].str.contains('3AM'),1,0)\n",
    "all_data['3:15AM']=np.where(all_data['TIME'].str.contains('3:15AM'),1,0)\n",
    "all_data['3:30AM']=np.where(all_data['TIME'].str.contains('3:30AM'),1,0)\n",
    "all_data['3:45AM']=np.where(all_data['TIME'].str.contains('3:45AM'),1,0)\n",
    "all_data['4AM']=np.where(all_data['TIME'].str.contains('4AM'),1,0)\n",
    "all_data['4:15AM']=np.where(all_data['TIME'].str.contains('4:15AM'),1,0)\n",
    "all_data['4:30AM']=np.where(all_data['TIME'].str.contains('4:30AM'),1,0)\n",
    "all_data['4:45AM']=np.where(all_data['TIME'].str.contains('4:45AM'),1,0)\n",
    "all_data['5AM']=np.where(all_data['TIME'].str.contains('5AM'),1,0)\n",
    "all_data['5:15AM']=np.where(all_data['TIME'].str.contains('5:15AM'),1,0)\n",
    "all_data['5:30AM']=np.where(all_data['TIME'].str.contains('5:30AM'),1,0)\n",
    "all_data['5:45AM']=np.where(all_data['TIME'].str.contains('5:45AM'),1,0)\n",
    "all_data['6AM']=np.where(all_data['TIME'].str.contains('6AM'),1,0)\n",
    "all_data['6:15AM']=np.where(all_data['TIME'].str.contains('6:15AM'),1,0)\n",
    "all_data['6:30AM']=np.where(all_data['TIME'].str.contains('6:30AM'),1,0)\n",
    "all_data['6:45AM']=np.where(all_data['TIME'].str.contains('6:45AM'),1,0)\n",
    "all_data['7AM']=np.where(all_data['TIME'].str.contains('7AM'),1,0)\n",
    "all_data['7:15AM']=np.where(all_data['TIME'].str.contains('7:15AM'),1,0)\n",
    "all_data['7:30AM']=np.where(all_data['TIME'].str.contains('7:30AM'),1,0)\n",
    "all_data['7:45AM']=np.where(all_data['TIME'].str.contains('7:45AM'),1,0)\n",
    "all_data['8AM']=np.where(all_data['TIME'].str.contains('8AM'),1,0)\n",
    "all_data['8:15AM']=np.where(all_data['TIME'].str.contains('8:15AM'),1,0)\n",
    "all_data['8:30AM']=np.where(all_data['TIME'].str.contains('8:30AM'),1,0)\n",
    "all_data['8:45AM']=np.where(all_data['TIME'].str.contains('8:45AM'),1,0)\n",
    "all_data['9AM']=np.where(all_data['TIME'].str.contains('9AM'),1,0)\n",
    "all_data['9:15AM']=np.where(all_data['TIME'].str.contains('9:15AM'),1,0)\n",
    "all_data['9:30AM']=np.where(all_data['TIME'].str.contains('9:30AM'),1,0)\n",
    "all_data['9:45AM']=np.where(all_data['TIME'].str.contains('9:45AM'),1,0)\n",
    "all_data['10AM']=np.where(all_data['TIME'].str.contains('10AM'),1,0)\n",
    "all_data['10:15AM']=np.where(all_data['TIME'].str.contains('10:15AM'),1,0)\n",
    "all_data['10:30AM']=np.where(all_data['TIME'].str.contains('10:30AM'),1,0)\n",
    "all_data['10:45AM']=np.where(all_data['TIME'].str.contains('10:45AM'),1,0)\n",
    "all_data['11AM']=np.where(all_data['TIME'].str.contains('11AM'),1,0)\n",
    "all_data['11:15AM']=np.where(all_data['TIME'].str.contains('11:15AM'),1,0)\n",
    "all_data['11:30AM']=np.where(all_data['TIME'].str.contains('11:30AM'),1,0)\n",
    "all_data['11:45AM']=np.where(all_data['TIME'].str.contains('11:45AM'),1,0)\n",
    "all_data['12AM']=np.where(all_data['TIME'].str.contains('12AM'),1,0)\n",
    "all_data['12:15AM']=np.where(all_data['TIME'].str.contains('12:15AM'),1,0)\n",
    "all_data['12:30AM']=np.where(all_data['TIME'].str.contains('12:30AM'),1,0)\n",
    "all_data['12:45AM']=np.where(all_data['TIME'].str.contains('12:45AM'),1,0)\n",
    "all_data['12NOON']=np.where(all_data['TIME'].str.contains('12NOON'),1,0)\n",
    "        \n",
    "                             \n",
    "all_data['1:15PM']=np.where(all_data['TIME'].str.contains('1:15PM'),1,0)\n",
    "all_data['1:30PM']=np.where(all_data['TIME'].str.contains('1:30PM'),1,0)\n",
    "all_data['1:45PM']=np.where(all_data['TIME'].str.contains('1:45PM'),1,0)\n",
    "all_data['2PM']=np.where(all_data['TIME'].str.contains('2PM'),1,0)\n",
    "all_data['2:15PM']=np.where(all_data['TIME'].str.contains('2:15PM'),1,0)\n",
    "all_data['2:30PM']=np.where(all_data['TIME'].str.contains('2:30PM'),1,0)\n",
    "all_data['2:45PM']=np.where(all_data['TIME'].str.contains('2:45PM'),1,0)\n",
    "all_data['3PM']=np.where(all_data['TIME'].str.contains('3PM'),1,0)\n",
    "all_data['3:15PM']=np.where(all_data['TIME'].str.contains('3:15PM'),1,0)\n",
    "all_data['3:30PM']=np.where(all_data['TIME'].str.contains('3:30PM'),1,0)\n",
    "all_data['3:45PM']=np.where(all_data['TIME'].str.contains('3:45PM'),1,0)\n",
    "all_data['4PM']=np.where(all_data['TIME'].str.contains('4PM'),1,0)\n",
    "all_data['4:15PM']=np.where(all_data['TIME'].str.contains('4:15PM'),1,0)\n",
    "all_data['4:30PM']=np.where(all_data['TIME'].str.contains('4:30PM'),1,0)\n",
    "all_data['4:45PM']=np.where(all_data['TIME'].str.contains('4:45PM'),1,0)\n",
    "all_data['5PM']=np.where(all_data['TIME'].str.contains('5PM'),1,0)\n",
    "all_data['5:15PM']=np.where(all_data['TIME'].str.contains('5:15PM'),1,0)\n",
    "all_data['5:30PM']=np.where(all_data['TIME'].str.contains('5:30PM'),1,0)\n",
    "all_data['5:45PM']=np.where(all_data['TIME'].str.contains('5:45PM'),1,0)\n",
    "all_data['6PM']=np.where(all_data['TIME'].str.contains('6PM'),1,0)\n",
    "all_data['6:15PM']=np.where(all_data['TIME'].str.contains('6:15PM'),1,0)\n",
    "all_data['6:30PM']=np.where(all_data['TIME'].str.contains('6:30PM'),1,0)\n",
    "all_data['6:45PM']=np.where(all_data['TIME'].str.contains('6:45PM'),1,0)\n",
    "all_data['7PM']=np.where(all_data['TIME'].str.contains('7PM'),1,0)\n",
    "all_data['7:15PM']=np.where(all_data['TIME'].str.contains('7:15PM'),1,0)\n",
    "all_data['7:30PM']=np.where(all_data['TIME'].str.contains('7:30PM'),1,0)\n",
    "all_data['7:45PM']=np.where(all_data['TIME'].str.contains('7:45PM'),1,0)\n",
    "all_data['8PM']=np.where(all_data['TIME'].str.contains('8PM'),1,0)\n",
    "all_data['8:15PM']=np.where(all_data['TIME'].str.contains('8:15PM'),1,0)\n",
    "all_data['8:30PM']=np.where(all_data['TIME'].str.contains('8:30PM'),1,0)\n",
    "all_data['8:45PM']=np.where(all_data['TIME'].str.contains('8:45PM'),1,0)\n",
    "all_data['9PM']=np.where(all_data['TIME'].str.contains('9PM'),1,0)\n",
    "all_data['9:15PM']=np.where(all_data['TIME'].str.contains('9:15PM'),1,0)\n",
    "all_data['9:30PM']=np.where(all_data['TIME'].str.contains('9:30PM'),1,0)\n",
    "all_data['9:45PM']=np.where(all_data['TIME'].str.contains('9:45PM'),1,0)\n",
    "all_data['10PM']=np.where(all_data['TIME'].str.contains('10PM'),1,0)\n",
    "all_data['10:15PM']=np.where(all_data['TIME'].str.contains('10:15PM'),1,0)\n",
    "all_data['10:30PM']=np.where(all_data['TIME'].str.contains('10:30PM'),1,0)\n",
    "all_data['10:45PM']=np.where(all_data['TIME'].str.contains('10:45PM'),1,0)\n",
    "all_data['11PM']=np.where(all_data['TIME'].str.contains('11PM'),1,0)\n",
    "all_data['11:15PM']=np.where(all_data['TIME'].str.contains('11:15PM'),1,0)\n",
    "all_data['11:30PM']=np.where(all_data['TIME'].str.contains('11:30PM'),1,0)\n",
    "all_data['11:45PM']=np.where(all_data['TIME'].str.contains('11:45PM'),1,0)\n",
    "all_data['12PM']=np.where(all_data['TIME'].str.contains('12PM'),1,0)\n",
    "all_data['12:15PM']=np.where(all_data['TIME'].str.contains('12:15PM'),1,0)\n",
    "all_data['12:30PM']=np.where(all_data['TIME'].str.contains('12:30PM'),1,0)\n",
    "all_data['12:45PM']=np.where(all_data['TIME'].str.contains('12:45PM'),1,0)\n",
    "all_data['12MIDNIGHT']=np.where(all_data['TIME'].str.contains('12MIDNIGHT'),1,0)\n",
    "\n",
    "all_data['CITY_LEN']=all_data['CITY'].str.len()\n",
    "all_data['LOCALITY_LEN']=all_data['LOCALITY'].str.len()\n",
    "all_data['CITY_wrd'] = all_data.CITY.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['LOCALITY_wrd'] = all_data.LOCALITY.apply(lambda x: len(str(x).split(' '))) \n",
    "all_data['CITY_no_count'] = all_data['CITY'].str.count(r'[0-9]')\n",
    "all_data['LOCALITY_no_count'] = all_data['LOCALITY'].str.count(r'[0-9]')\n",
    "all_data['CITY_str_count'] = all_data['CITY'].str.count(r'[A-Z]')\n",
    "all_data['LOCALITY_str_count'] = all_data['LOCALITY'].str.count(r'[A-Z]')\n",
    "\n",
    "all_data['TITLE_LEN']=all_data['TITLE'].str.len()\n",
    "all_data['CUISINES_LEN']=all_data['CUISINES'].str.len()\n",
    "all_data['TITLE_wrd1'] = all_data.TITLE.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['CUISINES_wrd1'] = all_data.CUISINES.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['TITLE_wrd2'] = all_data.TITLE.apply(lambda x: len(str(x).split(',')))\n",
    "all_data['CUISINES_wrd2'] = all_data.CUISINES.apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "del all_data['TITLE']\n",
    "del all_data['CUISINES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16921, 293)\n"
     ]
    }
   ],
   "source": [
    "all_data=all_data.loc[:, (all_data != 0).any(axis=0)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_data['TIME']\n",
    "del all_data['CITY']\n",
    "del all_data['LOCALITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_new=pd.concat((feature_data1,all_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contains NaN/Empty cells :  True\n",
      "\n",
      "Contains NaN/Empty cells :  False\n",
      "(16921, 307)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContains NaN/Empty cells : \", all_data_new.isnull().values.any())\n",
    "all_data_new.fillna(0, inplace = True)\n",
    "print(\"\\nContains NaN/Empty cells : \", all_data_new.isnull().values.any())\n",
    "all_data_new=all_data_new.loc[:, (all_data_new != 0).any(axis=0)]\n",
    "print(all_data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1=all_data_new[:12690]\n",
    "test1=all_data_new[12690:16921]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(train1, target, train_size=0.7, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.95,\n",
       "       colsample_bytree=0.95, gamma=1, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=12, min_child_weight=1, missing=None, n_estimators=600,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=99, silent=True,\n",
       "       subsample=0.95)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=xgb.XGBRegressor(colsample_bylevel=0.95, \n",
    "                       colsample_bytree=0.95,\n",
    "                       gamma=1, #1\n",
    "                       learning_rate=0.1, #0.1\n",
    "                       max_depth=12,#10\n",
    "                       min_child_weight=1, \n",
    "                       n_estimators=600, #500\n",
    "                       objective='reg:linear', \n",
    "                       reg_alpha=1, \n",
    "                       reg_lambda=1,\n",
    "                       scale_pos_weight=1, \n",
    "                       seed=99, \n",
    "                       subsample=0.95,\n",
    "                       silent=True)\n",
    "model.fit(train1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8413852791745227\n",
      "0.8488313212436474\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_validation,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2872276472574732\n",
      "0.2800656214418674\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_train)\n",
    "ypred_val = model.predict(X_validation)\n",
    "\n",
    "\n",
    "print(sqrt(mean_squared_error(y_train, ypred)))\n",
    "print(sqrt(mean_squared_error(y_validation, ypred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262.3697322879204\n"
     ]
    }
   ],
   "source": [
    "yall = model.predict(train1)\n",
    "yall=pd.DataFrame(yall)\n",
    "yall.columns=['COST1']\n",
    "yall[\"COST\"] = np.expm1(yall[\"COST1\"])\n",
    "\n",
    "print(sqrt(mean_squared_error(tg.COST, yall.COST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COST1</th>\n",
       "      <th>COST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.469296</td>\n",
       "      <td>644.029785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.264420</td>\n",
       "      <td>1427.556885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.357709</td>\n",
       "      <td>575.923096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.239618</td>\n",
       "      <td>511.662781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.890768</td>\n",
       "      <td>360.682800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      COST1         COST\n",
       "0  6.469296   644.029785\n",
       "1  7.264420  1427.556885\n",
       "2  6.357709   575.923096\n",
       "3  6.239618   511.662781\n",
       "4  5.890768   360.682800"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COST</th>\n",
       "      <th>COST1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200</td>\n",
       "      <td>7.090910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "      <td>7.313887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800</td>\n",
       "      <td>6.685861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>6.685861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>5.707110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COST     COST1\n",
       "0  1200  7.090910\n",
       "1  1500  7.313887\n",
       "2   800  6.685861\n",
       "3   800  6.685861\n",
       "4   300  5.707110"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1=model.feature_importances_\n",
    "t1=pd.DataFrame(t1)\n",
    "\n",
    "t2=train1.columns\n",
    "t2=pd.DataFrame(t2)\n",
    "\n",
    "t1.columns=['Importance']\n",
    "t2.columns=['Variable']\n",
    "\n",
    "t3=pd.concat([t2,t1],axis=1)\n",
    "t4=t3[t3['Importance'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CUISINES</th>\n",
       "      <th>TIME</th>\n",
       "      <th>CITY</th>\n",
       "      <th>LOCALITY</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VOTES</th>\n",
       "      <th>cat_num_feat0</th>\n",
       "      <th>cat_num_feat1</th>\n",
       "      <th>cat_num_feat2</th>\n",
       "      <th>...</th>\n",
       "      <th>LOCALITY_wrd</th>\n",
       "      <th>LOCALITY_no_count</th>\n",
       "      <th>CITY_str_count</th>\n",
       "      <th>LOCALITY_str_count</th>\n",
       "      <th>TITLE_LEN</th>\n",
       "      <th>CUISINES_LEN</th>\n",
       "      <th>TITLE_wrd1</th>\n",
       "      <th>CUISINES_wrd1</th>\n",
       "      <th>TITLE_wrd2</th>\n",
       "      <th>CUISINES_wrd2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.290172</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>3.6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3.776566</td>\n",
       "      <td>526.625712</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027362</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.170264</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>4.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.062472</td>\n",
       "      <td>952.233708</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.290172</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.170264</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>3.8</td>\n",
       "      <td>221.0</td>\n",
       "      <td>3.776566</td>\n",
       "      <td>526.625712</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.332959</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.137773</td>\n",
       "      <td>0.004407</td>\n",
       "      <td>4.1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.652441</td>\n",
       "      <td>228.451993</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030022</td>\n",
       "      <td>0.006914</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.137773</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>3.8</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3.899335</td>\n",
       "      <td>225.771619</td>\n",
       "      <td>3.898980</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TITLE  CUISINES      TIME      CITY  LOCALITY  RATING  VOTES  \\\n",
       "0  0.290172  0.000059  0.000059  0.007035  0.002144     3.6   49.0   \n",
       "1  0.027362  0.000059  0.001596  0.170264  0.003216     4.2   30.0   \n",
       "2  0.290172  0.000236  0.001005  0.170264  0.000715     3.8  221.0   \n",
       "3  0.332959  0.000177  0.002600  0.137773  0.004407     4.1   24.0   \n",
       "4  0.030022  0.006914  0.009101  0.137773  0.004823     3.8  165.0   \n",
       "\n",
       "   cat_num_feat0  cat_num_feat1  cat_num_feat2      ...        LOCALITY_wrd  \\\n",
       "0       3.776566     526.625712       3.600000      ...                   2   \n",
       "1       4.062472     952.233708       4.200000      ...                   1   \n",
       "2       3.776566     526.625712       3.650000      ...                   1   \n",
       "3       3.652441     228.451993       4.066667      ...                   2   \n",
       "4       3.899335     225.771619       3.898980      ...                   2   \n",
       "\n",
       "   LOCALITY_no_count  CITY_str_count  LOCALITY_str_count  TITLE_LEN  \\\n",
       "0                0.0             5.0                13.0         13   \n",
       "1                0.0             7.0                 9.0         17   \n",
       "2                0.0             7.0                10.0         13   \n",
       "3                0.0             6.0                10.0         11   \n",
       "4                0.0             6.0                10.0         14   \n",
       "\n",
       "   CUISINES_LEN  TITLE_wrd1  CUISINES_wrd1  TITLE_wrd2  CUISINES_wrd2  \n",
       "0            27           2              4           1              3  \n",
       "1            30           2              4           2              3  \n",
       "2            42           2              5           1              4  \n",
       "3            16           2              2           1              2  \n",
       "4             8           2              1           1              1  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cols = list(t4.Variable)\n",
    "all_data_new_v1 = all_data_new[my_cols]\n",
    "all_data_new_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1=all_data_new_v1[:12690]\n",
    "test1=all_data_new_v1[12690:16921]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(train1, target, train_size=0.7, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.95,\n",
       "       colsample_bytree=0.95, gamma=1, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=12, min_child_weight=1, missing=None, n_estimators=600,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=99, silent=True,\n",
       "       subsample=0.95)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=xgb.XGBRegressor(colsample_bylevel=0.95, \n",
    "                       colsample_bytree=0.95,\n",
    "                       gamma=1, #1\n",
    "                       learning_rate=0.1, #0.1\n",
    "                       max_depth=12,#10\n",
    "                       min_child_weight=1, \n",
    "                       n_estimators=600, #500\n",
    "                       objective='reg:linear', \n",
    "                       reg_alpha=1, \n",
    "                       reg_lambda=1,\n",
    "                       scale_pos_weight=1, \n",
    "                       seed=99, \n",
    "                       subsample=0.95,\n",
    "                       silent=True)\n",
    "model.fit(train1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8409652101811458\n",
      "0.8482477731603794\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_validation,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2876077369769492\n",
      "0.2806056616802737\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X_train)\n",
    "ypred_val = model.predict(X_validation)\n",
    "\n",
    "\n",
    "print(sqrt(mean_squared_error(y_train, ypred)))\n",
    "print(sqrt(mean_squared_error(y_validation, ypred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.63896315021003\n"
     ]
    }
   ],
   "source": [
    "yall = model.predict(train1)\n",
    "yall=pd.DataFrame(yall)\n",
    "yall.columns=['COST1']\n",
    "yall[\"COST\"] = np.expm1(yall[\"COST1\"])\n",
    "\n",
    "print(sqrt(mean_squared_error(tg.COST, yall.COST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_sub=model.predict(test1)\n",
    "y_sub=pd.DataFrame(y_sub)\n",
    "y_sub.columns=['COST1']\n",
    "y_sub[\"COST\"] = np.expm1(y_sub[\"COST1\"])\n",
    "\n",
    "del y_sub[\"COST1\"]\n",
    "\n",
    "y_sub.to_excel('sub_xgb_freq_m_vs_log.xlsx', index=False)#0.8507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"xgb_final_log.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test2=model.predict(test1)\n",
    "y_pred_test2=pd.DataFrame(y_pred_test2)\n",
    "y_pred_test2.columns=['xgb1']\n",
    "y_pred_test2[\"xgb\"] = np.expm1(y_pred_test2[\"xgb1\"])\n",
    "del y_pred_test2[\"xgb1\"]\n",
    "y_pred_test2.to_excel('submission_XGBoost_stck_val_log.xlsx', index=False)\n",
    "\n",
    "y_pred_test2=model.predict(train1)\n",
    "y_pred_test2=pd.DataFrame(y_pred_test2)\n",
    "y_pred_test2.columns=['xgb1']\n",
    "y_pred_test2[\"xgb\"] = np.expm1(y_pred_test2[\"xgb1\"])\n",
    "del y_pred_test2[\"xgb1\"]\n",
    "y_pred_test2.to_excel('submission_XGBoost_stck_train_log.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
