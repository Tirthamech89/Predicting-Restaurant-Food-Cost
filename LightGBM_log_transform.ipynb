{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('Data_Train.xlsx')\n",
    "test = pd.read_excel('Data_Test.xlsx')\n",
    "\n",
    "tg=train[['COST']]\n",
    "tg[\"COST1\"] = np.log1p(tg[\"COST\"])\n",
    "target=tg.COST1\n",
    "del train['COST']\n",
    "del train['RESTAURANT_ID']\n",
    "del test['RESTAURANT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data=pd.concat([train,test])\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['TITLE']=all_data['TITLE'].str.strip()\n",
    "all_data['TITLE']=all_data['TITLE'].str.upper()\n",
    "\n",
    "all_data['CUISINES']=all_data['CUISINES'].str.strip()\n",
    "all_data['CUISINES']=all_data['CUISINES'].str.upper()\n",
    "\n",
    "all_data['CITY']=all_data['CITY'].str.strip()\n",
    "all_data['CITY']=all_data['CITY'].str.upper()\n",
    "\n",
    "all_data['LOCALITY']=all_data['LOCALITY'].str.strip()\n",
    "all_data['LOCALITY']=all_data['LOCALITY'].str.upper()\n",
    "\n",
    "all_data['TIME']=all_data['TIME'].str.strip()\n",
    "all_data['TIME']=all_data['TIME'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Rating\n",
    "\n",
    "rates = list(all_data['RATING'])\n",
    "\n",
    "for i in range(len(rates)) :\n",
    "    try:\n",
    "       rates[i] = float(rates[i])\n",
    "    except :\n",
    "       rates[i] = np.nan\n",
    "\n",
    "\n",
    "# Votes\n",
    "       \n",
    "votes = list(all_data['VOTES'])\n",
    "\n",
    "for i in range(len(votes)) :\n",
    "    try:\n",
    "       votes[i] = int(votes[i].split(\" \")[0].strip())\n",
    "    except :\n",
    "       pass     \n",
    "\n",
    "num_data = {}\n",
    "\n",
    "num_data['RATING'] = rates\n",
    "num_data['VOTES'] = votes\n",
    "\n",
    "num_data = pd.DataFrame(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_data['RATING']\n",
    "del all_data['VOTES']\n",
    "feature_data=pd.concat([all_data,num_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TITLE', 'CUISINES', 'TIME', 'CITY', 'LOCALITY']\n",
      "['RATING', 'VOTES']\n"
     ]
    }
   ],
   "source": [
    "cat_cols=feature_data.columns[feature_data.dtypes=='object'].tolist()\n",
    "print (cat_cols)\n",
    "\n",
    "num_cols=feature_data.columns[feature_data.dtypes!='object'].tolist()\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_num_feats=pd.DataFrame(np.column_stack([feature_data[m[0]].map(dict(feature_data.groupby(m[0])[m[1]].mean()))\n",
    " for m in [(a,b) for a in cat_cols for b in num_cols]]),\n",
    "columns=['cat_num_feat'+str(i) for i in range(len(cat_cols)*len(num_cols))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_data.reset_index(drop=True,inplace=True)\n",
    "feature_data=pd.concat((feature_data,cat_num_feats),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoding(BaseEstimator):\n",
    "    categorical_columns = None\n",
    "    return_df = False\n",
    "    random_state = 30\n",
    "    threshold = 50\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert_input(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            if isinstance(X, list):\n",
    "                X = pd.DataFrame(np.array(X))\n",
    "            elif isinstance(X, (np.generic, np.ndarray, pd.Series)):\n",
    "                X = pd.DataFrame(X)\n",
    "            else:\n",
    "                raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "            X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "        x = X.copy(deep = True)\n",
    "        return x\n",
    "\n",
    "    def get_categorical_columns(self, X):\n",
    "        return X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    def get_numerical_columns(self,X):\n",
    "        temp_x=X[X.columns[X.nunique()<=self.threshold]]\n",
    "        col_names=temp_x.columns[temp_x.dtypes!='object']\n",
    "        return col_names\n",
    "\n",
    "    def apply_encoding(self, X_in, encoding_dict):\n",
    "        X = self.convert_input(X_in)\n",
    "        for col in self.categorical_columns:\n",
    "            if col in encoding_dict:\n",
    "                freq_dict = encoding_dict[col]\n",
    "                X[col] = X[col].apply(lambda x: freq_dict[x] if x  in freq_dict else np.nan)\n",
    "        return X\n",
    "\n",
    "    def create_encoding_dict(self, X, y):\n",
    "        return {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if X is None:\n",
    "            raise ValueError(\"Input array is required to call fit method!\")\n",
    "        X = self.convert_input(X)\n",
    "        self.encoding_dict = self.create_encoding_dict(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = self.apply_encoding(X, self.encoding_dict)\n",
    "        if self.return_df:\n",
    "            return df\n",
    "        else:\n",
    "            return df.values\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X = self.convert_input(X)\n",
    "        for col in self.categorical_columns:\n",
    "            freq_dict = self.encoding_dict[col]\n",
    "            for key, val in freq_dict.iteritems():\n",
    "                X.loc[X[col] == val, col] = key\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FreqeuncyEncoding(Encoding):\n",
    "    '''\n",
    "    class to perform FreqeuncyEncoding on Categorical Variables\n",
    "    Initialization Variabes:\n",
    "    categorical_columns: list of categorical columns from the dataframe\n",
    "    or list of indexes of caategorical columns for numpy ndarray\n",
    "    return_df: boolean\n",
    "        if True: returns pandas dataframe on transformation\n",
    "        else: return numpy ndarray\n",
    "    '''\n",
    "    def __init__(self, categorical_columns = None, return_df = False):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.return_df = return_df\n",
    "\n",
    "    def create_encoding_dict(self, X, y):\n",
    "        encoding_dict = {}\n",
    "        if self.categorical_columns is None:\n",
    "            self.categorical_columns = self.get_categorical_columns(X)\n",
    "        for col in self.categorical_columns:\n",
    "            encoding_dict.update({col: X[col].value_counts(normalize = True).to_dict()})\n",
    "        return encoding_dict\n",
    "\n",
    "\n",
    "fe=FreqeuncyEncoding(categorical_columns=cat_cols,return_df=True)\n",
    "feature_data1=fe.fit_transform(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximum Titles in a Cell :  2\n",
      "\n",
      "\n",
      "Number of Unique Titles :  25\n",
      "\n",
      "\n",
      "Unique Titles:\n",
      " ['CASUAL DINING' 'BAR' 'QUICK BITES' 'DESSERT PARLOR' 'CAFE'\n",
      " 'MICROBREWERY' 'BEVERAGE SHOP' 'IRANI CAFE' 'BAKERY' 'NONE' 'PUB'\n",
      " 'FINE DINING' 'SWEET SHOP' 'LOUNGE' 'FOOD COURT' 'FOOD TRUCK' 'MESS'\n",
      " 'KIOSK' 'CLUB' 'CONFECTIONERY' 'DHABA' 'MEAT SHOP' 'COCKTAIL BAR'\n",
      " 'PAAN SHOP' 'BHOJANALYA']\n"
     ]
    }
   ],
   "source": [
    "titles = list(all_data['TITLE'])\n",
    "\n",
    "# Finding Maximum number of titles mentioned in a single cell\n",
    "maxim = 1\n",
    "for i in titles :\n",
    "    if len(i.split(',')) > maxim:\n",
    "         maxim = len(i.split(','))\n",
    "         \n",
    "print(\"\\n\\nMaximum Titles in a Cell : \", maxim)    \n",
    "\n",
    "all_titles = []\n",
    "\n",
    "for i in titles :\n",
    "    if len(i.split(',')) == 1:\n",
    "         all_titles.append(i.split(',')[0].strip().upper())\n",
    "    else :\n",
    "        for it in range(len(i.split(','))):\n",
    "            all_titles.append(i.split(',')[it].strip().upper())\n",
    "\n",
    "print(\"\\n\\nNumber of Unique Titles : \", len(pd.Series(all_titles).unique()))\n",
    "print(\"\\n\\nUnique Titles:\\n\", pd.Series(all_titles).unique())\n",
    "\n",
    "all_titles = list(pd.Series(all_titles).unique())\n",
    "\n",
    "\n",
    "for i in range(25):\n",
    "    ttl=all_titles[i]\n",
    "    ttl1=ttl+'1'\n",
    "    all_data[ttl1] = all_data['TITLE'].str.contains(ttl)\n",
    "    all_data[ttl1] = all_data[ttl1].map({True: 1, False: 0})\n",
    "    \n",
    "    \n",
    "#del all_data['TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximum cuisines in a Cell :  8\n",
      "\n",
      "\n",
      "Number of Unique Cuisines :  130\n",
      "\n",
      "\n",
      "Unique Cuisines:\n",
      " ['MALWANI' 'GOAN' 'NORTH INDIAN' 'ASIAN' 'MODERN INDIAN' 'JAPANESE'\n",
      " 'CHINESE' 'BIRYANI' 'HYDERABADI' 'TIBETAN' 'DESSERTS' 'SEAFOOD' 'CAFE'\n",
      " 'PIZZA' 'BURGER' 'BAR FOOD' 'SOUTH INDIAN' 'FAST FOOD' 'BEVERAGES'\n",
      " 'ARABIAN' 'MUGHLAI' 'MAHARASHTRIAN' 'PARSI' 'THAI' 'BAKERY' 'MOMOS'\n",
      " 'CONTINENTAL' 'EUROPEAN' 'ROLLS' 'ANDHRA' 'ITALIAN' 'BBQ' 'FINGER FOOD'\n",
      " 'TEA' 'AMERICAN' 'HEALTHY FOOD' 'COFFEE' 'INDONESIAN' 'KOREAN' 'NEPALESE'\n",
      " 'ICE CREAM' 'MEXICAN' 'KERALA' 'INDIAN' 'MITHAI' 'STREET FOOD'\n",
      " 'MALAYSIAN' 'VIETNAMESE' 'IRANIAN' 'KEBAB' 'JUICES' 'SANDWICH'\n",
      " 'MEDITERRANEAN' 'SALAD' 'GUJARATI' 'RAJASTHANI' 'TEX-MEX' 'ROAST CHICKEN'\n",
      " 'BURMESE' 'CHETTINAD' 'NORTH EASTERN' 'LEBANESE' 'COFFEE AND TEA' 'GRILL'\n",
      " '' 'BIHARI' 'BENGALI' 'LUCKNOWI' 'AWADHI' 'STEAK' 'FRENCH' 'PORTUGUESE'\n",
      " 'WRAPS' 'SRI LANKAN' 'ORIYA' 'ETHIOPIAN' 'KONKAN' 'SUSHI' 'SPANISH'\n",
      " 'RUSSIAN' 'MANGALOREAN' 'TURKISH' 'BUBBLE TEA' 'AFGHAN' 'NAGA'\n",
      " 'SINGAPOREAN' 'GERMAN' 'MIDDLE EASTERN' 'SINDHI' 'CANTONESE' 'HOT POT'\n",
      " 'PAN ASIAN' 'SATAY' 'DUMPLINGS' 'KASHMIRI' 'RAW MEATS' 'DRINKS ONLY'\n",
      " 'MOROCCAN' 'PANINI' 'CAFE FOOD' 'CHARCOAL CHICKEN' 'BELGIAN' 'MONGOLIAN'\n",
      " 'TAMIL' 'AFRICAN' 'PAAN' 'ASSAMESE' 'HOT DOGS' 'POKE' 'BRITISH' 'BOHRI'\n",
      " 'FUSION' 'ARMENIAN' 'SOUTH AMERICAN' 'GREEK' 'PAKISTANI' 'PERUVIAN'\n",
      " 'CUISINE VARIES' 'IRISH' 'MULTI CUISINE' 'JEWISH' 'VEGAN' 'ORIENTAL'\n",
      " 'MODERN AUSTRALIAN' 'EGYPTIAN' 'FISH AND CHIPS' 'BRAZILIAN' 'MISHTI'\n",
      " 'FALAFEL' 'HAWAIIAN']\n"
     ]
    }
   ],
   "source": [
    "# Analysing cuisines \n",
    "\n",
    "cuisines = list(all_data['CUISINES'])\n",
    "\n",
    "maxim = 1\n",
    "for i in cuisines :\n",
    "    if len(i.split(',')) > maxim:\n",
    "         maxim = len(i.split(','))\n",
    "         \n",
    "print(\"\\n\\nMaximum cuisines in a Cell : \", maxim)    \n",
    "\n",
    "all_cuisines = []\n",
    "\n",
    "for i in cuisines :\n",
    "    if len(i.split(',')) == 1:\n",
    "         #print(i.split(',')[0])\n",
    "         all_cuisines.append(i.split(',')[0].strip().upper())\n",
    "    else :\n",
    "        for it in range(len(i.split(','))):\n",
    "            #print(i.split(',')[it])\n",
    "            all_cuisines.append(i.split(',')[it].strip().upper())\n",
    "\n",
    "print(\"\\n\\nNumber of Unique Cuisines : \", len(pd.Series(all_cuisines).unique()))\n",
    "print(\"\\n\\nUnique Cuisines:\\n\", pd.Series(all_cuisines).unique())\n",
    "\n",
    "all_cuisines = list(pd.Series(all_cuisines).unique())\n",
    "\n",
    "for i in range(130):\n",
    "    ttl=all_cuisines[i]\n",
    "    ttl2=ttl+'2'\n",
    "    all_data[ttl2] = all_data['CUISINES'].str.contains(ttl)\n",
    "    all_data[ttl2] = all_data[ttl2].map({True: 1, False: 0})\n",
    "    \n",
    "    \n",
    "#del all_data['CUISINES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16921, 160)\n"
     ]
    }
   ],
   "source": [
    "all_data=all_data.loc[:, (all_data != 0).any(axis=0)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  import sys\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "all_data['MON-TUE']=np.where(all_data['TIME'].str.contains('(MON-TUE)'),1,0)\n",
    "all_data['MON-WED']=np.where(all_data['TIME'].str.contains('(MON-WED)'),1,0)\n",
    "all_data['MON-THU']=np.where(all_data['TIME'].str.contains('(MON-THU)'),1,0)\n",
    "all_data['MON-FRI']=np.where(all_data['TIME'].str.contains('(MON-FRI)'),1,0)\n",
    "all_data['MON-SAT']=np.where(all_data['TIME'].str.contains('(MON-SAT)'),1,0)\n",
    "all_data['MON-SUN']=np.where(all_data['TIME'].str.contains('(MON-SUN)'),1,0)\n",
    "all_data['TUE-WED']=np.where(all_data['TIME'].str.contains('(TUE-WED)'),1,0)\n",
    "all_data['TUE-THU']=np.where(all_data['TIME'].str.contains('(TUE-THU)'),1,0)\n",
    "all_data['TUE-FRI']=np.where(all_data['TIME'].str.contains('(TUE-FRI)'),1,0)\n",
    "all_data['TUE-SAT']=np.where(all_data['TIME'].str.contains('(TUE-SAT)'),1,0)\n",
    "all_data['TUE-SUN']=np.where(all_data['TIME'].str.contains('(TUE-SUN)'),1,0)\n",
    "all_data['WED-THU']=np.where(all_data['TIME'].str.contains('(WED-THU)'),1,0)\n",
    "all_data['WED-FRI']=np.where(all_data['TIME'].str.contains('(WED-FRI)'),1,0)\n",
    "all_data['WED-SAT']=np.where(all_data['TIME'].str.contains('(WED-SAT)'),1,0)\n",
    "all_data['WED-SUN']=np.where(all_data['TIME'].str.contains('(WED-SUN)'),1,0)\n",
    "all_data['THU-FRI']=np.where(all_data['TIME'].str.contains('(THU-FRI)'),1,0)\n",
    "all_data['THU-SAT']=np.where(all_data['TIME'].str.contains('(THU-SAT)'),1,0)\n",
    "all_data['THU-SUN']=np.where(all_data['TIME'].str.contains('(THU-SUN)'),1,0)\n",
    "all_data['FRI-SAT']=np.where(all_data['TIME'].str.contains('(FRI-SAT)'),1,0)\n",
    "all_data['FRI-SUN']=np.where(all_data['TIME'].str.contains('(FRI-SUN)'),1,0)\n",
    "all_data['SAT-SUN']=np.where(all_data['TIME'].str.contains('(SAT-SUN)'),1,0)\n",
    "\n",
    "all_data['MON']=np.where(all_data['TIME'].str.contains('MON'),1,0)\n",
    "all_data['TUE']=np.where(all_data['TIME'].str.contains('TUE'),1,0)\n",
    "all_data['WED']=np.where(all_data['TIME'].str.contains('WED'),1,0)\n",
    "all_data['THU']=np.where(all_data['TIME'].str.contains('THU'),1,0)\n",
    "all_data['FRI']=np.where(all_data['TIME'].str.contains('FRI'),1,0)\n",
    "all_data['SAT']=np.where(all_data['TIME'].str.contains('SAT'),1,0)\n",
    "all_data['SUN']=np.where(all_data['TIME'].str.contains('SUN'),1,0)\n",
    "\n",
    "all_data['AM']=np.where(all_data['TIME'].str.contains('AM'),1,0)\n",
    "all_data['PM']=np.where(all_data['TIME'].str.contains('PM'),1,0)\n",
    "all_data['AM_cnt']=all_data['TIME'].str.count('AM')\n",
    "all_data['PM_cnt']=all_data['TIME'].str.count('PM')\n",
    "all_data['NOON']=np.where(all_data['TIME'].str.contains('NOON'),1,0)\n",
    "all_data['MIDNIGHT']=np.where(all_data['TIME'].str.contains('MIDNIGHT'),1,0)\n",
    "all_data['CLOSED']=np.where(all_data['TIME'].str.contains('CLOSED'),1,0)\n",
    "all_data['HOURS']=np.where(all_data['TIME'].str.contains('HOURS'),1,0)\n",
    "all_data['TIME_CNT']=all_data['TIME'].str.len()\n",
    "all_data['comma_count'] = all_data['TIME'].str.count(',')\n",
    "all_data['dash_count'] = all_data['TIME'].str.count('-')\n",
    "all_data['collon_count'] = all_data['TIME'].str.count(':')\n",
    "all_data['dotted_count'] = all_data['TIME'].str.count('...')\n",
    "#all_data['TIME_no_count'] = all_data['TIME'].str.count(r'[0-9]')\n",
    "#all_data['TIME_str_count'] = all_data['TIME'].str.count(r'[A-Z]')\n",
    "\n",
    "all_data['1:15AM']=np.where(all_data['TIME'].str.contains('1:15AM'),1,0)\n",
    "all_data['1:30AM']=np.where(all_data['TIME'].str.contains('1:30AM'),1,0)\n",
    "all_data['1:45AM']=np.where(all_data['TIME'].str.contains('1:45AM'),1,0)\n",
    "all_data['2AM']=np.where(all_data['TIME'].str.contains('2AM'),1,0)\n",
    "all_data['2:15AM']=np.where(all_data['TIME'].str.contains('2:15AM'),1,0)\n",
    "all_data['2:30AM']=np.where(all_data['TIME'].str.contains('2:30AM'),1,0)\n",
    "all_data['2:45AM']=np.where(all_data['TIME'].str.contains('2:45AM'),1,0)\n",
    "all_data['3AM']=np.where(all_data['TIME'].str.contains('3AM'),1,0)\n",
    "all_data['3:15AM']=np.where(all_data['TIME'].str.contains('3:15AM'),1,0)\n",
    "all_data['3:30AM']=np.where(all_data['TIME'].str.contains('3:30AM'),1,0)\n",
    "all_data['3:45AM']=np.where(all_data['TIME'].str.contains('3:45AM'),1,0)\n",
    "all_data['4AM']=np.where(all_data['TIME'].str.contains('4AM'),1,0)\n",
    "all_data['4:15AM']=np.where(all_data['TIME'].str.contains('4:15AM'),1,0)\n",
    "all_data['4:30AM']=np.where(all_data['TIME'].str.contains('4:30AM'),1,0)\n",
    "all_data['4:45AM']=np.where(all_data['TIME'].str.contains('4:45AM'),1,0)\n",
    "all_data['5AM']=np.where(all_data['TIME'].str.contains('5AM'),1,0)\n",
    "all_data['5:15AM']=np.where(all_data['TIME'].str.contains('5:15AM'),1,0)\n",
    "all_data['5:30AM']=np.where(all_data['TIME'].str.contains('5:30AM'),1,0)\n",
    "all_data['5:45AM']=np.where(all_data['TIME'].str.contains('5:45AM'),1,0)\n",
    "all_data['6AM']=np.where(all_data['TIME'].str.contains('6AM'),1,0)\n",
    "all_data['6:15AM']=np.where(all_data['TIME'].str.contains('6:15AM'),1,0)\n",
    "all_data['6:30AM']=np.where(all_data['TIME'].str.contains('6:30AM'),1,0)\n",
    "all_data['6:45AM']=np.where(all_data['TIME'].str.contains('6:45AM'),1,0)\n",
    "all_data['7AM']=np.where(all_data['TIME'].str.contains('7AM'),1,0)\n",
    "all_data['7:15AM']=np.where(all_data['TIME'].str.contains('7:15AM'),1,0)\n",
    "all_data['7:30AM']=np.where(all_data['TIME'].str.contains('7:30AM'),1,0)\n",
    "all_data['7:45AM']=np.where(all_data['TIME'].str.contains('7:45AM'),1,0)\n",
    "all_data['8AM']=np.where(all_data['TIME'].str.contains('8AM'),1,0)\n",
    "all_data['8:15AM']=np.where(all_data['TIME'].str.contains('8:15AM'),1,0)\n",
    "all_data['8:30AM']=np.where(all_data['TIME'].str.contains('8:30AM'),1,0)\n",
    "all_data['8:45AM']=np.where(all_data['TIME'].str.contains('8:45AM'),1,0)\n",
    "all_data['9AM']=np.where(all_data['TIME'].str.contains('9AM'),1,0)\n",
    "all_data['9:15AM']=np.where(all_data['TIME'].str.contains('9:15AM'),1,0)\n",
    "all_data['9:30AM']=np.where(all_data['TIME'].str.contains('9:30AM'),1,0)\n",
    "all_data['9:45AM']=np.where(all_data['TIME'].str.contains('9:45AM'),1,0)\n",
    "all_data['10AM']=np.where(all_data['TIME'].str.contains('10AM'),1,0)\n",
    "all_data['10:15AM']=np.where(all_data['TIME'].str.contains('10:15AM'),1,0)\n",
    "all_data['10:30AM']=np.where(all_data['TIME'].str.contains('10:30AM'),1,0)\n",
    "all_data['10:45AM']=np.where(all_data['TIME'].str.contains('10:45AM'),1,0)\n",
    "all_data['11AM']=np.where(all_data['TIME'].str.contains('11AM'),1,0)\n",
    "all_data['11:15AM']=np.where(all_data['TIME'].str.contains('11:15AM'),1,0)\n",
    "all_data['11:30AM']=np.where(all_data['TIME'].str.contains('11:30AM'),1,0)\n",
    "all_data['11:45AM']=np.where(all_data['TIME'].str.contains('11:45AM'),1,0)\n",
    "all_data['12AM']=np.where(all_data['TIME'].str.contains('12AM'),1,0)\n",
    "all_data['12:15AM']=np.where(all_data['TIME'].str.contains('12:15AM'),1,0)\n",
    "all_data['12:30AM']=np.where(all_data['TIME'].str.contains('12:30AM'),1,0)\n",
    "all_data['12:45AM']=np.where(all_data['TIME'].str.contains('12:45AM'),1,0)\n",
    "all_data['12NOON']=np.where(all_data['TIME'].str.contains('12NOON'),1,0)\n",
    "        \n",
    "                             \n",
    "all_data['1:15PM']=np.where(all_data['TIME'].str.contains('1:15PM'),1,0)\n",
    "all_data['1:30PM']=np.where(all_data['TIME'].str.contains('1:30PM'),1,0)\n",
    "all_data['1:45PM']=np.where(all_data['TIME'].str.contains('1:45PM'),1,0)\n",
    "all_data['2PM']=np.where(all_data['TIME'].str.contains('2PM'),1,0)\n",
    "all_data['2:15PM']=np.where(all_data['TIME'].str.contains('2:15PM'),1,0)\n",
    "all_data['2:30PM']=np.where(all_data['TIME'].str.contains('2:30PM'),1,0)\n",
    "all_data['2:45PM']=np.where(all_data['TIME'].str.contains('2:45PM'),1,0)\n",
    "all_data['3PM']=np.where(all_data['TIME'].str.contains('3PM'),1,0)\n",
    "all_data['3:15PM']=np.where(all_data['TIME'].str.contains('3:15PM'),1,0)\n",
    "all_data['3:30PM']=np.where(all_data['TIME'].str.contains('3:30PM'),1,0)\n",
    "all_data['3:45PM']=np.where(all_data['TIME'].str.contains('3:45PM'),1,0)\n",
    "all_data['4PM']=np.where(all_data['TIME'].str.contains('4PM'),1,0)\n",
    "all_data['4:15PM']=np.where(all_data['TIME'].str.contains('4:15PM'),1,0)\n",
    "all_data['4:30PM']=np.where(all_data['TIME'].str.contains('4:30PM'),1,0)\n",
    "all_data['4:45PM']=np.where(all_data['TIME'].str.contains('4:45PM'),1,0)\n",
    "all_data['5PM']=np.where(all_data['TIME'].str.contains('5PM'),1,0)\n",
    "all_data['5:15PM']=np.where(all_data['TIME'].str.contains('5:15PM'),1,0)\n",
    "all_data['5:30PM']=np.where(all_data['TIME'].str.contains('5:30PM'),1,0)\n",
    "all_data['5:45PM']=np.where(all_data['TIME'].str.contains('5:45PM'),1,0)\n",
    "all_data['6PM']=np.where(all_data['TIME'].str.contains('6PM'),1,0)\n",
    "all_data['6:15PM']=np.where(all_data['TIME'].str.contains('6:15PM'),1,0)\n",
    "all_data['6:30PM']=np.where(all_data['TIME'].str.contains('6:30PM'),1,0)\n",
    "all_data['6:45PM']=np.where(all_data['TIME'].str.contains('6:45PM'),1,0)\n",
    "all_data['7PM']=np.where(all_data['TIME'].str.contains('7PM'),1,0)\n",
    "all_data['7:15PM']=np.where(all_data['TIME'].str.contains('7:15PM'),1,0)\n",
    "all_data['7:30PM']=np.where(all_data['TIME'].str.contains('7:30PM'),1,0)\n",
    "all_data['7:45PM']=np.where(all_data['TIME'].str.contains('7:45PM'),1,0)\n",
    "all_data['8PM']=np.where(all_data['TIME'].str.contains('8PM'),1,0)\n",
    "all_data['8:15PM']=np.where(all_data['TIME'].str.contains('8:15PM'),1,0)\n",
    "all_data['8:30PM']=np.where(all_data['TIME'].str.contains('8:30PM'),1,0)\n",
    "all_data['8:45PM']=np.where(all_data['TIME'].str.contains('8:45PM'),1,0)\n",
    "all_data['9PM']=np.where(all_data['TIME'].str.contains('9PM'),1,0)\n",
    "all_data['9:15PM']=np.where(all_data['TIME'].str.contains('9:15PM'),1,0)\n",
    "all_data['9:30PM']=np.where(all_data['TIME'].str.contains('9:30PM'),1,0)\n",
    "all_data['9:45PM']=np.where(all_data['TIME'].str.contains('9:45PM'),1,0)\n",
    "all_data['10PM']=np.where(all_data['TIME'].str.contains('10PM'),1,0)\n",
    "all_data['10:15PM']=np.where(all_data['TIME'].str.contains('10:15PM'),1,0)\n",
    "all_data['10:30PM']=np.where(all_data['TIME'].str.contains('10:30PM'),1,0)\n",
    "all_data['10:45PM']=np.where(all_data['TIME'].str.contains('10:45PM'),1,0)\n",
    "all_data['11PM']=np.where(all_data['TIME'].str.contains('11PM'),1,0)\n",
    "all_data['11:15PM']=np.where(all_data['TIME'].str.contains('11:15PM'),1,0)\n",
    "all_data['11:30PM']=np.where(all_data['TIME'].str.contains('11:30PM'),1,0)\n",
    "all_data['11:45PM']=np.where(all_data['TIME'].str.contains('11:45PM'),1,0)\n",
    "all_data['12PM']=np.where(all_data['TIME'].str.contains('12PM'),1,0)\n",
    "all_data['12:15PM']=np.where(all_data['TIME'].str.contains('12:15PM'),1,0)\n",
    "all_data['12:30PM']=np.where(all_data['TIME'].str.contains('12:30PM'),1,0)\n",
    "all_data['12:45PM']=np.where(all_data['TIME'].str.contains('12:45PM'),1,0)\n",
    "all_data['12MIDNIGHT']=np.where(all_data['TIME'].str.contains('12MIDNIGHT'),1,0)\n",
    "\n",
    "all_data['CITY_LEN']=all_data['CITY'].str.len()\n",
    "all_data['LOCALITY_LEN']=all_data['LOCALITY'].str.len()\n",
    "all_data['CITY_wrd'] = all_data.CITY.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['LOCALITY_wrd'] = all_data.LOCALITY.apply(lambda x: len(str(x).split(' '))) \n",
    "all_data['CITY_no_count'] = all_data['CITY'].str.count(r'[0-9]')\n",
    "all_data['LOCALITY_no_count'] = all_data['LOCALITY'].str.count(r'[0-9]')\n",
    "all_data['CITY_str_count'] = all_data['CITY'].str.count(r'[A-Z]')\n",
    "all_data['LOCALITY_str_count'] = all_data['LOCALITY'].str.count(r'[A-Z]')\n",
    "\n",
    "all_data['TITLE_LEN']=all_data['TITLE'].str.len()\n",
    "all_data['CUISINES_LEN']=all_data['CUISINES'].str.len()\n",
    "all_data['TITLE_wrd1'] = all_data.TITLE.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['CUISINES_wrd1'] = all_data.CUISINES.apply(lambda x: len(str(x).split(' ')))\n",
    "all_data['TITLE_wrd2'] = all_data.TITLE.apply(lambda x: len(str(x).split(',')))\n",
    "all_data['CUISINES_wrd2'] = all_data.CUISINES.apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "del all_data['TITLE']\n",
    "del all_data['CUISINES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16921, 293)\n"
     ]
    }
   ],
   "source": [
    "all_data=all_data.loc[:, (all_data != 0).any(axis=0)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del all_data['TIME']\n",
    "del all_data['CITY']\n",
    "del all_data['LOCALITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_new=pd.concat((feature_data1,all_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contains NaN/Empty cells :  True\n",
      "\n",
      "Contains NaN/Empty cells :  False\n",
      "(16921, 307)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContains NaN/Empty cells : \", all_data_new.isnull().values.any())\n",
    "all_data_new.fillna(0, inplace = True)\n",
    "print(\"\\nContains NaN/Empty cells : \", all_data_new.isnull().values.any())\n",
    "all_data_new=all_data_new.loc[:, (all_data_new != 0).any(axis=0)]\n",
    "print(all_data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1=all_data_new[:12690]\n",
    "test1=all_data_new[12690:16921]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(train1, target, train_size=0.7, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = train1.columns.tolist()\n",
    "\n",
    "# LightGBM dataset formatting \n",
    "lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                feature_name=feature_names)\n",
    "lgvalid = lgb.Dataset(X_validation, y_validation,\n",
    "                feature_name=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgtall = lgb.Dataset(train1, target,\n",
    "                feature_name=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pf416e\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[500]\ttrain's rmse: 0.139763\tvalid's rmse: 0.133789\n",
      "[1000]\ttrain's rmse: 0.0679046\tvalid's rmse: 0.0646077\n",
      "[1500]\ttrain's rmse: 0.034467\tvalid's rmse: 0.0327563\n",
      "[2000]\ttrain's rmse: 0.0189989\tvalid's rmse: 0.017825\n",
      "[2500]\ttrain's rmse: 0.0121435\tvalid's rmse: 0.0109232\n",
      "[3000]\ttrain's rmse: 0.00930055\tvalid's rmse: 0.00811166\n",
      "[3500]\ttrain's rmse: 0.00850692\tvalid's rmse: 0.00718294\n",
      "[4000]\ttrain's rmse: 0.00824762\tvalid's rmse: 0.00691135\n",
      "[4500]\ttrain's rmse: 0.0081711\tvalid's rmse: 0.00681892\n",
      "[5000]\ttrain's rmse: 0.00814935\tvalid's rmse: 0.00679497\n",
      "[5500]\ttrain's rmse: 0.00814281\tvalid's rmse: 0.00678469\n",
      "[6000]\ttrain's rmse: 0.00814081\tvalid's rmse: 0.00678198\n",
      "[6500]\ttrain's rmse: 0.00814023\tvalid's rmse: 0.00678106\n",
      "[7000]\ttrain's rmse: 0.00814012\tvalid's rmse: 0.00678065\n",
      "[7500]\ttrain's rmse: 0.00814004\tvalid's rmse: 0.00678064\n",
      "Early stopping, best iteration is:\n",
      "[7177]\ttrain's rmse: 0.00814012\tvalid's rmse: 0.00678051\n",
      "RMSE of the validation set: 0.006780511847326791\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective' : 'regression',\n",
    "    'metric' : 'rmse',\n",
    "    'num_leaves' : 50, \n",
    "    'max_depth': 15,  \n",
    "    'learning_rate' : 0.1,\n",
    "    'feature_fraction' : 0.9,\n",
    "    'verbosity' : -1,\n",
    "    'feature_fraction_seed': 1234,\n",
    "    'bagging_seed': 1234,\n",
    "    'colsample_bytree': 0.99,\n",
    "    'max_bin': 256,\n",
    "    'num_iterations': 10000,\n",
    "    #'reg_alpha': 5,\n",
    "    #'reg_lambda': 10,\n",
    "    #'min_split_gain': 0.4,\n",
    "    'min_child_weight': 2,\n",
    "    'min_child_samples':4,\n",
    "}\n",
    "\n",
    "\n",
    "lgb_clf = lgb.train(\n",
    "    params,\n",
    "    lgtall,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=[lgtrain, lgvalid],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    early_stopping_rounds=500,\n",
    "    verbose_eval=500)\n",
    "\n",
    "print(\"RMSE of the validation set:\", np.sqrt(mean_squared_error(y_validation, lgb_clf.predict(X_validation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008140122407075975\n",
      "0.006780511847326791\n"
     ]
    }
   ],
   "source": [
    "ypred = lgb_clf.predict(X_train, num_iteration=lgb_clf.best_iteration)\n",
    "ypred_val = lgb_clf.predict(X_validation, num_iteration=lgb_clf.best_iteration)\n",
    "\n",
    "print(sqrt(mean_squared_error(y_train, ypred)))\n",
    "print(sqrt(mean_squared_error(y_validation, ypred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1949415084571267\n"
     ]
    }
   ],
   "source": [
    "yall = lgb_clf.predict(data=train1,num_iteration=lgb_clf.best_iteration)\n",
    "yall=pd.DataFrame(yall)\n",
    "yall.columns=['COST1']\n",
    "yall[\"COST\"] = np.expm1(yall[\"COST1\"])\n",
    "\n",
    "print(sqrt(mean_squared_error(tg.COST, yall.COST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all=lgb_clf.predict(data=test1,num_iteration=lgb_clf.best_iteration)\n",
    "y_all=pd.DataFrame(y_all)\n",
    "y_all.columns=['COST1']\n",
    "y_all[\"COST\"] = np.expm1(y_all[\"COST1\"])\n",
    "\n",
    "del y_all[\"COST1\"]\n",
    "\n",
    "y_all.to_excel('submission_lightGBM1_log.xlsx', index=False)#0.8447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x19f99cb44a8>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf.save_model('model_lgb_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking\n",
    "\n",
    "y_all=lgb_clf.predict(data=test1,num_iteration=lgb_clf.best_iteration)\n",
    "y_all=pd.DataFrame(y_all)\n",
    "y_all.columns=['lgb1']\n",
    "y_all[\"lgb\"] = np.expm1(y_all[\"lgb1\"])\n",
    "del y_all[\"lgb1\"]\n",
    "\n",
    "y_all.to_excel('submission_lightGBM_stck_val_log.xlsx', index=False)\n",
    "\n",
    "y_all=lgb_clf.predict(data=train1,num_iteration=lgb_clf.best_iteration)\n",
    "y_all=pd.DataFrame(y_all)\n",
    "y_all.columns=['lgb1']\n",
    "y_all[\"lgb\"] = np.expm1(y_all[\"lgb1\"])\n",
    "del y_all[\"lgb1\"]\n",
    "\n",
    "y_all.to_excel('submission_lightGBM_stck_train_log.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
